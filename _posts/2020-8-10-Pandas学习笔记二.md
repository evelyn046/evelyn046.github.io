---
title: Pandas 学习笔记（二）
author: Evelyn
date: 2020-08-17 10:16:00 +0800
categories: [学习笔记, Python]
tags: [Python, 数据分析, Pandas]
---


# 09 Pandas的axis参数

- axis=0或者"index"：
  - 如果是单行操作，就指的是某一行
  - 如果是聚合操作，指的是跨行cross rows
- axis=1或者"columns"：
  - 如果是单列操作，就指的是某一列
  - 如果是聚合操作，指的是跨列cross columns

> 聚合操作：指定哪个维度，哪个维度动

![]({{ "/assets/img/2020-08-10-14-59-56.png" |  }})

### 单列drop，就是删除某一列

```
# 删除A列
df.drop("A", axis=1)
```

### 单行drop，就是删除某一行
```
# 删除index=1的行
df.drop(1, axis=0)
```

### 按axis=0/index执行mean聚合操作

```
# axis=0 or axis=index
df.mean(axis=0)
```
![]({{ "/assets/img/2020-08-10-15-05-43.png" |  }})

![]({{ "/assets/img/2020-08-10-15-05-59.png" |  }})

### 按axis=1/columns执行mean聚合操作
```
df.mean(axis=1)
```

![]({{ "/assets/img/2020-08-10-15-06-48.png" |  }})

![]({{ "/assets/img/2020-08-10-15-07-23.png" |  }})

# 10 Index的用途

把数据存储于普通的column列也能用于数据查询，那使用index有什么好处？
- 更方便的数据查询；
- 使用index可以获得性能提升；
  - 如果index是唯一的，Pandas会使用哈希表优化，查询性能为O(1);
  - 如果index不是唯一的，但是有序，Pandas会使用二分查找算法，查询性能为O(logN);
  - 如果index是完全随机的，那么每次查询都要扫描全表，查询性能为O(N);
  
  ![]({{ "/assets/img/2020-08-10-15-50-22.png" |  }})

- 自动的数据对齐功能；
- 更多更强大的数据结构支持；
  - CategoricalIndex，基于分类数据的Index，提升性能；
  - MultiIndex，多维索引，用于groupby多维聚合后结果等；
  - DatetimeIndex，时间类型索引，强大的日期和时间的方法支持；

# 11 Pandas的Merge语法

Pandas的Merge，相当于Sql的Join，将不同的表按key关联到一个表

merge的语法：
```
pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('x', 'y'), copy=True, indicator=False, validate=None)
```
- left，right：要merge的dataframe或者有name的Series
- how：join类型，'left', 'right', 'outer', 'inner'
- on：join的key，left和right都需要有这个key
- left_on：left的df或者series的key
- right_on：right的df或者seires的key
- left_index，right_index：使用index而不是普通的column做join
- suffixes：两个元素的后缀，如果列有重名，自动添加后缀，默认是('x', 'y')

>[官方文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)

Merge时的数量关系
- one-to-one：一对一关系，关联的key都是唯一的
    比如(学号，姓名) merge (学号，年龄)
    结果条数为：1*1
- one-to-many：一对多关系，左边唯一key，右边不唯一key
    比如(学号，姓名) merge (学号，[语文成绩、数学成绩、英语成绩])
    结果条数为：1*N
- many-to-many：多对多关系，左边右边都不是唯一的
    比如（学号，[语文成绩、数学成绩、英语成绩]） merge (学号，[篮球、足球、乒乓球])
    结果条数为：M*N

left join、right join、inner join、outer join的区别

![]({{ "/assets/img/2020-08-10-16-37-56.png" |  }})

- inner join，默认：左边和右边的key都有，才会出现在结果里
- left join：左边的都会出现在结果里，右边的如果无法匹配则为Null
- right join：右边的都会出现在结果里，左边的如果无法匹配则为Null
- outer join¶：左边、右边的都会出现在结果里，如果无法匹配则为Null

如果出现非Key的字段重名怎么办？

pandas默认加后缀

![]({{ "/assets/img/2020-08-10-17-35-59.png" |  }})

也可以使用suffixes参数指定后缀

```
pd.merge(left, right, on='key', suffixes=('_left', '_right'))
```
![]({{ "/assets/img/2020-08-10-17-36-35.png" |  }})

# 12 Pandas实现数据的合并concat
使用场景：
批量合并相同格式的Excel、给DataFrame添加行、给DataFrame添加列

concat语法：
- 使用某种合并方式(inner/outer)
- 沿着某个轴向(axis=0/1)
- 把多个Pandas对象(DataFrame/Series)合并成一个。

concat语法：``pandas.concat(objs, axis=0, join='outer', ignore_index=False)``
- objs：一个列表，内容可以是DataFrame或者Series，可以混合
- axis：默认是0代表按行合并，如果等于1代表按列合并
- join：合并的时候索引的对齐方式，默认是outer join，也可以是inner join
- ignore_index：是否忽略掉原来的数据索引

append语法：``DataFrame.append(other, ignore_index=False)``

append只有按行合并，没有按列合并，相当于concat按行的简写形式 （把一个表格直接复制到另一个表格下面）

- other：单个dataframe、series、dict，或者列表
- ignore_index：是否忽略掉原来的数据索引

参考文档：
- [pandas.concat的api文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)
- [pandas.concat的教程](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)
- [pandas.append的api文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)

### 使用pandas.concat合并数据
![]({{ "/assets/img/2020-08-11-10-42-14.png" |  }})  ![]({{ "/assets/img/2020-08-11-10-42-29.png" |  }})

- 默认的concat，参数为axis=0、join=outer、ignore_index=False
    ```
    pd.concat([df1,df2])
    ```
    ![]({{ "/assets/img/2020-08-11-10-43-48.png" |  }})

- 使用ignore_index=True可以忽略原来的索引
    ```
    pd.concat([df1,df2], ignore_index=True)
    ```
    ![]({{ "/assets/img/2020-08-11-10-45-04.png" |  }})

- 使用join=inner过滤掉不匹配的列
  
    ```
    pd.concat([df1,df2], ignore_index=True, join="inner")
    ```
    ![]({{ "/assets/img/2020-08-11-10-45-44.png" |  }})

- 使用axis=1相当于添加新列
    ```
    # 添加一列Series
    s1 = pd.Series(list(range(4)), name="F")
    pd.concat([df1,s1], axis=1)
    ```
    ![]({{ "/assets/img/2020-08-11-10-47-04.png" |  }})

    ```
    # 添加多列Series
    pd.concat([df1,s1,s2], axis=1)

    # 列表可以只有Series
    pd.concat([s1,s2], axis=1)

    # 列表是可以混合顺序的
    pd.concat([s1,df1,s2], axis=1)

    ```

### 使用DataFrame.append()按行合并数据

- 给1个dataframe添加另一个dataframe
- 忽略原来的索引ignore_index=True
- 一行一行的给DataFrame添加数据
  ```
  #低性能版本
  for i in range(5):
    # 注意这里每次都在复制
    df = df.append({'A': i}, ignore_index=True)
 
  #高性能版本
  # 第一个入参是一个列表，避免了多次复制
    pd.concat(
        [pd.DataFrame([i], columns=['A']) for i in range(5)],
        ignore_index=True
    )
  ```

# 13 Pandas批量拆分Excel与合并Excel

```
# 创建两个路径，一个是源文件所在路径，一个是拆分文件存放的路径
work_dir="./course_datas/c15_excel_split_merge"
splits_dir=f"{work_dir}/splits"

# 如果拆分文件存放的路径不存在，则新建路径
import os
if not os.path.exists(splits_dir):
    os.mkdir(splits_dir)

# 读取大Excel到pandas

import pandas as pd 
df source = pd.read_ecel(f"{work_dir}/crazyant_blog_articles_source.xlsx")

# 拆分文件前需要知道文件一共有多少行
# 使用df.source.shape 返回值(row,column)
# .shape有两个元素，读取第一个元素
total_row_count = df_source.shape[0]


# 将一个大Excel等份拆成多个小Excel
# step 1: 使用df.iloc方法，将一个大的dataframe，拆分成多个小dataframe
# step 2: 将使用dataframe.to_excel保存每个小Excel

# 计算拆分后的每个excel行数

## 这个大excel，会拆分给这几个人
user_names = ["xiao_shuai", "xiao_wang", "xiao_ming", "xiao_lei", "xiao_bo", "xiao_hong"]

## 每个人的任务数目
split_size = total_row_count // len(user_names)
if total_row_count % len(user_names) != 0:
    split_size += 1

# 拆成多个dataframe
df_subs = []
# idf：每个用户的索引，从0开始
for idx, user_name in enumerate(user_names):
    # iloc的开始索引
    begin = idx*split_size
    
    # iloc的结束索引
    end = begin+split_size

    # 实现df按照iloc拆分
    df_sub = df_source.iloc[begin:end]

    # 将每个人的df存入列表
    df_subs.append((idx, user_name, df_sub))

# 将每个dataframe存入excel
for idx, user_name, df_sub in df_subs:
    file_name = f"{splits_dir}/crazyant_blog_articles_{idx}_{user_name}.xlsx"
    df_sub.to_excel(file_name, index=False)


# 合并多个小excel到一个大excel
<!-- 
step 1: 遍历文件夹，得到要合并的Excel文件列表
step 2: 分别读取到dataframe，给每个df添加一列用于标记来源
step 3: 使用pd.concat进行df批量合并
step 4: 将合并后的dataframe输出到excel -->

# 遍历文件夹，得到要合并的Excel名称列表
impor os
excel_names = []
for excel_name in os.listdir(split_dir):
    excel_names.append(excel_name)

# 分别读取到dataframe

df_list=[]
for excel_name in excel_names:
    #读取每个excel到df
    excel_path = f"{splits_dir}/{excel_name}"
    df_split = pd.read_excel(excel_path)
    # 提取username
    username = excel_name.repalce("crazyant_blog_articles_","")[2:].replace(".xlsx","")
    df_split["username"] = username

    df_list.append(df_split)

# 使用pd.concat进行合并
df_merge = pd.concat(df_list)

# 将合并后的dataframe输出到excel
df_merged.to_excel(f"{work_dir}/crazyant_blog_articles_merged.xlsx", index=False)
```

# 14 Pandas如何实现groupby分组统计
![]({{ "/assets/img/2020-08-11-17-53-44.png" |  }})

(1) 分组使用聚合函数做数据统计
- 单个列groupby，查询所有数据列的统计
  ```
  df.groupby("A").sum()
  ```
  ![]({{ "/assets/img/2020-08-11-17-55-28.png" |  }})

  - groupby中的'A'变成了数据的索引列
  - 因为要统计sum，但B列不是数字，所以被自动忽略掉
  
- 多个列groupby，查询所有数据列的统计
  ```
  df.groupby(['A','B']).mean()
  ```
  ![]({{ "/assets/img/2020-08-11-17-56-43.png" |  }})
  A B列变成了二级索引

  ```
  df.groupby(['A','B'], as_index=False).mean()
  ```
  ![]({{ "/assets/img/2020-08-11-17-57-42.png" |  }})

- 同时查看多种数据统计(groupby.agg)
  ```
  df.groupby('A').agg([np.sum, np.mean, np.std])
  ```
  ![]({{ "/assets/img/2020-08-11-17-59-06.png" |  }})
  
  列变成了多级索引
- 查看单列的结果数据统计
  ```
  # 方法1：预过滤，性能更好
  df.groupby('A')['C'].agg([np.sum, np.mean, np.std])
  # 方法2
  df.groupby('A').agg([np.sum, np.mean, np.std])['C']
  ```
  ![]({{ "/assets/img/2020-08-11-18-00-35.png" |  }})

- 不同列使用不同的聚合函数
  ```
  df.groupby('A').agg({"C":np.sum, "D":np.mean})
  ```

# 15 Pandas的分层索引MultiIndx

作用：

* 分层索引：在一个轴向上拥有多个索引层级，可以表达更高维度数据的形式；
* 可以更方便的进行数据筛选，如果有序则性能更好；
* groupby等操作的结果，如果是多KEY，结果是分层索引，需要会使用
* 一般不需要自己创建分层索引(MultiIndex有构造函数但一般不用)

### Series的分层索引MultiIndex

对df使用groupby，结果是多维索引（多维索引中，空白的意思是：使用上面的值）

![]({{ "/assets/img/2020-08-12-17-02-27.png" |  }})

- ``ser.unstack()``可以实现索引降维（将二级索引变成列名）

![]({{ "/assets/img/2020-08-12-17-03-44.png" |  }})

- ``ser.reset_index()``可以将两层索引都变为普通的列，将series变成df

![]({{ "/assets/img/2020-08-12-17-05-38.png" |  }})

###Series有多层索引MultiIndex怎样筛选数据？

- 筛选第一级索引  ``ser.loc['一级索引值']``
- 筛选多层索引，用元组的形式筛选  ``ser.loc[('一级索引值', '二级索引值')]``
- 筛选二级索引  ``ser.loc[:, '二级索引值']``

### DataFrame的多层索引MultiIndex

![]({{ "/assets/img/2020-08-12-17-12-18.png" |  }})

使用``stocks.set_index(['公司', '日期'], inplace=True)``，将公司列和日期列设置为分层索引

![]({{ "/assets/img/2020-08-12-17-12-54.png" |  }})

### DataFrame有多层索引MultiIndex怎样筛选数据？

【***补充***】在选择数据时： 
* 元组(key1,key2)代表筛选多层索引，其中key1是索引第一级，key2是第二级，比如key1=JD, key2=2019-10-02
* 列表[key1,key2]代表同一层的多个KEY，其中key1和key2是并列的同级索引，比如key1=JD, key2=BIDU

- .loc首个值是元组，多级索引筛选
  ``stocks.loc['BIDU']``
  
  ![]({{ "/assets/img/2020-08-12-17-25-33.png" |  }})

  ``stocks.loc[('BIDU', '2019-10-02'), :]``或``stocks.loc[('BIDU', '2019-10-02'), '开盘']``指定单个列

  ![]({{ "/assets/img/2020-08-12-17-26-19.png" |  }})

- .loc首个值是列表，同级索引筛选(并列筛选)
  
  ``stocks.loc[['BIDU', 'JD'], :]``

  ![]({{ "/assets/img/2020-08-12-17-29-02.png" |  }})

- 混合
  
  ``stocks.loc[(['BIDU', 'JD'], '2019-10-03'), :]``

  ![]({{ "/assets/img/2020-08-12-17-29-37.png" |  }})

- 元组中的数据可以是单个值，也可以是一个列表
  
  ``stocks.loc[('BIDU', ['2019-10-02', '2019-10-03']), '收盘']``

  ![]({{ "/assets/img/2020-08-12-17-32-45.png" |  }})
  
  ```
  # slice(None)代表筛选这一索引的所有内容
  stocks.loc[(slice(None), ['2019-10-02', '2019-10-03']), :]
  ```
  ![]({{ "/assets/img/2020-08-12-17-33-40.png" |  }})

# 16 Pandas的数据转换函数

数据转换函数对比：map、apply、applymap：
1. map：只用于Series，实现每个值->值的映射；
2. apply：用于Series实现每个值的处理，用于Dataframe实现某个轴的Series的处理；
3. applymap：只能用于DataFrame，用于处理该DataFrame的每个元素；

### map
- Series.map(dict)
- Series.map(function)
  function的参数是Series的每个元素的值

### apply用于Series和DataFrame的转换

* Series.apply(function), 函数的参数是每个值
* DataFrame.apply(function), 函数的参数是Series

### applymap用于DataFrame所有值的转换

实现将股票代码英文转换成中文名字的四种方法
![]({{ "/assets/img/2020-08-12-17-58-34.png" |  }})
```
# 公司股票代码到中文的映射，注意这里是小写
dict_company_names = {
    "bidu": "百度",
    "baba": "阿里巴巴",
    "iq": "爱奇艺", 
    "jd": "京东"
}

# 方法1 Series.map(dict)
stocks["公司中文1"] = stocks["公司"].str.lower().map(dict_company_names)

# 方法2 Series.map(function)
stocks["公司中文2"] = stocks["公司"].map(lambda x : dict_company_names[x.lower()])

# 方法3 Series.apply(function)，function的参数是Series的每个值
stocks["公司中文3"] = stocks["公司"].apply(
    lambda x : dict_company_names[x.lower()])

# 方法4  DataFrame.apply(function)，function的参数是对应轴的Series
stocks["公司中文4"] = stocks.apply(
    lambda x : dict_company_names[x["公司"].lower()], 
    axis=1)

<!-- 注意这个代码：
1、apply是在stocks这个DataFrame上调用；
2、lambda x的x是一个Series，因为指定了axis=1所以Seires的key是列名，可以用x['公司']获取 -->

```
![]({{ "/assets/img/2020-08-12-18-00-59.png" |  }})

# 17 Pnadas怎样对每个分组应用apply函数
（按分组进行数据转换）
Pandas的GroupBy遵从split、apply、combine模式

![]({{ "/assets/img/2020-08-16-09-58-53.png" |  }})

这里的split指的是pandas的groupby，我们自己实现apply函数，apply返回的结果由pandas进行combine得到结果

GroupBy.apply(function)  
* function的第一个参数是dataframe
* function的返回结果，可是dataframe、series、单个值，甚至和输入dataframe完全没关系

### 对数值列按分组进行归一化

![]({{ "/assets/img/2020-08-16-10-00-13.png" |  }})

![]({{ "/assets/img/2020-08-16-10-04-59.png" |  }})

```
# 实现按照用户ID分组，然后对其中一列归一化
def ratings_norm(df):

    min_value = df["Rating"].min()
    max_value = df["Rating"].max()
    df["Rating_norm"] = df["Rating"].apply(
        lambda x:(x-min_value)/(max_value-min_value)
    return df
    )

ratings = ratings.groupby("UserID").apply(rating_norm)
```
```
ratings[ratings["UserID"]==1].head()
```

![]({{ "/assets/img/2020-08-16-10-09-35.png" |  }})

### 如何获取每个分组的TOPN数据

![]({{ "/assets/img/2020-08-16-10-10-45.png" |  }})

```
# 按照月份groupby，求每个分组温度最高的n个数据

def getWenduTopN(df,topn):

    #这里的df，是每个月份分组group的df

    return df.sort_values(by = "bWendu")[["ymd","bWendu"]][-topn:]

df.groupby("month").apply(getWenduTopN, topn=1).head()

```

![]({{ "/assets/img/2020-08-16-10-14-50.png" |  }})

grouby的apply函数返回的dataframe，其实和原来的dataframe其实可以完全不一样


# 18 Pandas使用stack和pivot实现数据透视

![]({{ "/assets/img/2020-08-16-10-21-29.png" |  }})

1. 经过统计得到多维度指标数据
2. 使用unstack实现数据二维透视
3. 使用pivot简化透视
4. stack、unstack、pivot的语法

### 经过统计得到多维度指标数据
(指定多个维度，计算聚合后的指标)

![]({{ "/assets/img/2020-08-16-10-24-58.png" |  }})

```
#统计得到“电影评分数据集”，每个月份的每个分数被评分多少次：（月份、分数1~5、次数）
#处理时间戳, unit='s'(“秒”的形式)
df["pdate"] = pd.to_datetime(df["Timestamp"], unit='s')

df_group = df.groupby([df["pdate"].dt.month,"Rating"])["UserID"].agg(pv=np.size)
```
![]({{ "/assets/img/2020-08-16-10-32-49.png" |  }})

![]({{ "/assets/img/2020-08-16-10-33-01.png" |  }})

对这样格式的数据，想查看按月份，不同评分的次数趋势，是没法实现的

需要将数据变换成每个评分是一列才可以实现

### 使用unstack实现数据二维透视

```
df_stack = df_group.unstack()
df_stack
```

![]({{ "/assets/img/2020-08-16-10-34-31.png" |  }})

```
# unstack和stack是互逆操作
df_stack.stack().head(20)
```

![]({{ "/assets/img/2020-08-16-10-35-14.png" |  }})

### 使用pivot简化透视

```
df_reset = df_group.reset_index()
df_reset.head()
```
![]({{ "/assets/img/2020-08-16-10-37-28.png" |  }})

```
df_pivot = df_reset.pivot("pdate", "Rating", "pv")
```

![]({{ "/assets/img/2020-08-16-10-38-03.png" |  }})

###  stack、unstack、pivot的语法

stack：DataFrame.stack(level=-1, dropna=True)，将column变成index，类似把横放的书籍变成竖放

level=-1代表多层索引的最内层，可以通过==0、1、2指定多层索引的对应层

![]({{ "/assets/img/2020-08-16-10-40-45.png" |  }})

unstack：DataFrame.unstack(level=-1, fill_value=None)，将index变成column，类似把竖放的书籍变成横放

![]({{ "/assets/img/2020-08-16-10-42-01.png" |  }})

pivot：DataFrame.pivot(index=None, columns=None, values=None)，指定index、columns、values实现二维透视

![]({{ "/assets/img/2020-08-16-10-42-29.png" |  }})

# 19 Pandas怎样实现对日期的快速处理

Pandas日期处理的作用：将2018-01-01、1/1/2018等多种日期格式映射成统一的格式对象，在该对象上提供强大的功能支持

几个概念：
1. pd.to_datetime：pandas的一个函数，能将字符串、列表、series变成日期形式
2. Timestamp：pandas表示日期的对象形式
3. DatetimeIndex：pandas表示日期的对象列表形式

其中：
* DatetimeIndex是Timestamp的列表形式
* pd.to_datetime对单个日期字符串处理会得到Timestamp
* pd.to_datetime对日期字符串列表处理会得到DatetimeIndex

![]({{ "/assets/img/2020-08-16-10-45-12.png" |  }})

# 20 Pandas怎么处理日期索引的缺失？

问题：按日期统计的数据，缺失了某天，导致数据不全该怎么补充日期？

![]({{ "/assets/img/2020-08-17-10-02-20.png" |  }})

1、DataFrame.reindex，调整dataframe的索引以适应新的索引  
2、DataFrame.resample，可以对时间序列重采样，支持补充缺失值

### 方法1：使用pandas.reindex方法

(1) 将df的字符串索引变成日期索引

```
#将pdate日期列设为索引
df_date = df.set_index("pdate")

#将df的索引设置为日期索引
df_date = df.set_index(pd.to_datetime(df_date.index))
```

(2)使用pandas.reindex填充缺失的索引

```
#生成完整的日期序列
pdates = pd.date_range(start="2019-12-01", end="2019-12-05")

df_date_new = df_date.redindex(pdates, fill_value=0)
```

![]({{ "/assets/img/2020-08-17-10-11-44.png" |  }})

### 使用pandas.resample方法

(1)将df的字符串索引变成日期索引
```
#简化写法
df_new2 = df.set_index(pd.to_datetime(df["pdate"])).drop("pdate", axis=1)
```

(2)使用dataframe的resample的方法按照天重采样
resample的含义：
改变数据的时间频率，比如把天数据变成月份，或者把小时数据变成分钟级别

resample的语法：
(DataFrame or Series).resample(arguments).(aggregate function)

[resample的采样规则参数](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)

```
# 由于采样会让区间变成一个值，所以需要指定mean等采样值的设定方法
df_new2 = df_new2.resample("D").mean().fillna(0)
```
